\chapter{Linear Quadratic Regulator}

The LQR controller is a standard baseline for regulating systems with multiple inputs and multiple outputs.
The controller requires knowledge of the system dynamics (state–space model), and two weighting matrices
in the cost function, which are design parameters chosen by the engineer.

LQR gained its position as a baseline method because it is:
\begin{itemize}
    \item mathematically well–defined with a unique optimal solution,
    \item robust for a wide class of linear systems,
    \item able to naturally balance performance vs.\ control effort,
    \item easy to tune once the engineer understands the role of $Q$ and $R$,
    \item computationally efficient (solved offline),
    \item superior to PID for multivariable, coupled, high-dimensional systems.
\end{itemize}

Why not PID? PID is fantastic for \textbf{single–input single–output} systems.  
But as soon as the dynamics are coupled (e.g.\ drones, robots, manipulators, cars),  
hand-tuning dozens of gains quickly becomes impossible.  
LQR solves all gains at once optimally — which is why it is the industry default for linear robotics control.


Assume we would like to control a 2D system: a moving sphere with inertia.

This system has two inputs, applied forces $u_x(n)$ and $u_y(n)$.  
The state is 4-dimensional: position and velocity in both spatial dimensions.
As shown in Fig.~\ref{fig:position_control},
the state vector is $x(n)$, and we explicitly mark the velocity term as $v(n)$
(which is part of the state).  
The control input $u(n)$ must counteract velocity and inertia to steer the system
into the desired state $x_r(n)$.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/examples/examples-trajectory_a.png}
    \caption{2D position control}
    \label{fig:position_control}
\end{figure}

This is a system with multiple outputs and multiple inputs:

\begin{align}
    x(n) &= 
    \begin{bmatrix} 
        p_x(n) \\ 
        p_y(n) \\ 
        v_x(n) \\ 
        v_y(n) 
    \end{bmatrix}
\end{align}

\begin{align}
    u(n) &= 
    \begin{bmatrix} 
        u_x(n) \\ 
        u_y(n)
    \end{bmatrix}
\end{align}

The ordering of state variables is arbitrary; here we place positions first, then velocities.


The intuition behind LQR is simple:

\textbf{Can we find a linear mapping from the error between the current state and desired state into an optimal control action?}

We define the tracking error:
\begin{align}
    e(n) = x_r(n) - x(n)
\end{align}

The simplest general mapping from error space to control space is matrix multiplication:

\begin{align}
    u(n) = K e(n)
\end{align}

The matrix $K$ is the gain matrix.  
It assigns weights to each state variable and uses a linear combination to compute the optimal action $u$.


\section{Derivation of LQR}

We now derive the matrix $K$.
We begin by defining notation and assumptions used throughout this derivation.

\begin{itemize}
    \item $n$ — discrete \textbf{time step}.
    \item $N$ — number of \textbf{state variables} (state dimension).
    \item $M$ — number of \textbf{control inputs} (input dimension).
    \item $x(n)$ — \textbf{system state}, column vector of size $N \times 1$.
    \item $u(n)$ — \textbf{control input}, column vector of size $M \times 1$.
    \item $A$ — \textbf{system matrix}, $N \times N$.
    \item $B$ — \textbf{input matrix}, $N \times M$.
    \item $Q$ — positive semidefinite \textbf{state weighting matrix}, $N \times N$.
    \item $R$ — positive semidefinite \textbf{input weighting matrix}, $M \times M$.
\end{itemize}

The controller drives the system toward the desired state $x_r(n)$ by minimizing the error  
$e(n) = x_r(n) - x(n)$.

The discrete–time system dynamics are:
\begin{align}
    x(n+1) = A x(n) + B u(n)
\end{align}


\subsection{Quadratic Cost Function}

The standard discrete-time LQR problem minimizes the infinite-horizon quadratic cost
\begin{align}
    \mathcal{L} = \sum_{n=0}^{\infty} \left[ x(n)^T Q x(n) + u(n)^T R u(n) \right]
\end{align}

This cost penalizes:
\begin{itemize}
    \item \textbf{state deviation} through $Q$ (how much we care about errors),
    \item \textbf{control effort} through $R$ (how expensive the control is).
\end{itemize}

Intuitively:
\begin{itemize}
    \item Large $Q$ entries $\Rightarrow$ state errors are very expensive.  
    \item Large $R$ entries $\Rightarrow$ control effort is expensive (fuel, power, torque limits).  
\end{itemize}

Fig.~\ref{fig:lqr_quadratic_loss} interprets the quadratic loss.  
For a 4-D state and 2-D control input, $Q$ is $4\times4$, $R$ is $2\times2$.
Multiplying a vector by its transpose forms a quadratic (parabolic) penalty —
the same idea as MSE loss.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-q_loss.png}
    \caption{Quadratic loss meaning}
    \label{fig:lqr_quadratic_loss}
\end{figure}

Choosing $Q$ and $R$ is a design choice.  
Most often they are diagonal, meaning each state variable is weighted independently.


\subsection{Solving the LQR Problem}

The optimal controller is obtained by solving the \textbf{discrete algebraic Riccati equation (DARE)}.
The solution yields the optimal gain matrix:

\begin{align}
    u(n) = -K x(n)
\end{align}

This steers the system to the origin $x=0$.  
To track non-zero references, we rewrite:

\begin{align}
    u(n) = K(x_r(n) - x(n))
\end{align}

(Here the minus sign is absorbed into the error definition.)

Fig.~\ref{fig:lqr_block_diagram} shows the flow diagram of basic LQR controller.
Fig.~\ref{fig:lqr_algo} shows the complete LQR algorithm. 


\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-lqr.png}
    \caption{LQR block diagram}
    \label{fig:lqr_block_diagram}
\end{figure}


\begin{enumerate}
    \item \textbf{Offline (precompute)}:
    \begin{enumerate}
        \item Given $A,\;B,\;Q,\;R$, solve the discrete algebraic Riccati equation (DARE) for $P$: \\
        \begin{align*}
            P &= A^\top P A - A^\top P B (R + B^\top P B)^{-1} B^\top P A + Q.
        \end{align*}
        \item Compute the optimal gain matrix $K$ : \\
        \begin{align*}
            K \;=\; (R + B^\top P B)^{-1} B^\top P A \; 
        \end{align*}
        \item \textbf{\textcolor{red}{Store}} $K$ and the nominal design matrices — these are used online.
    \end{enumerate}

    \item \textbf{Online (every control time-step $n$)}:
    \begin{enumerate}
        \item Measure or estimate current state $x(n)$ and set reference $x_r(n)$.
        \item Compute tracking error:
        \begin{align*}
            e(n) = x_r(n) - x(n).
        \end{align*}
        \item Compute control action:
        \begin{align*}
            u(n) = K\, e(n).
        \end{align*}
        \item Apply $u(n)$ to the plant. \\
        \textit{Notes:} $K$ is constant for linear time-invariant designs; computing it offline makes the real-time loop very cheap.
    \end{enumerate}
\end{enumerate}

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-lqr_algorithm.png}
    \caption{LQR algorithm}
    \label{fig:lqr_algo}
\end{figure}

\newpage
\section{Adding Integral Action}

The basic LQR has no integral term; thus it cannot remove steady-state error under constant disturbance.
Integral action is necessary when:
\begin{itemize}
    \item the plant has no natural integrator,
    \item external disturbances are constant or slowly varying,
    \item we need zero steady-state error for positions.
\end{itemize}

The integral term accumulates error over time:
\[
\textbf{if error persists, push harder}
\]

The structure of extended LQR with intagral action is in the Fig.~\ref{fig:lqri_block_diagram}.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-lqri_diagram.png}
    \caption{LQR with integral action block diagram}
    \label{fig:lqri_block_diagram}
\end{figure}

We augment the system with integral states:

\begin{align}
    \tilde{A} &= 
    \begin{bmatrix} 
        A & 0 \\
        I & I
    \end{bmatrix}
\end{align}

The cost matrix must be augmented as well:

\begin{align}
    \tilde{Q} &= 
    \begin{bmatrix} 
        0 & 0 \\
        0 & Q
    \end{bmatrix}
\end{align}

Solving DARE for the augmented system yields:

\begin{align}
    \tilde{K} &= 
    \begin{bmatrix} 
        K & K_i
    \end{bmatrix}
\end{align}

Fig.~\ref{fig:lqr_algo_integral_action} shows the algorithmic structure.



\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-lqri_algorithm.png}
    \caption{LQR algorithm with integral action}
    \label{fig:lqr_algo_integral_action}
\end{figure}

The implemented integral loop:
\begin{align}
    e(n) &= x_r(n) - x(n)\\
    e_{int}(n) &= e_{int}(n-1) + K_i e(n)\\
    u(n) &= -Kx(n) + e_{int}(n)
\end{align}

Note: the operator $K$ transforms state $x(n)$ and opertator $K_i$ is related with error signal $e(n)$.

\newpage
\section{Adding Antiwindup}

Real actuators are limited.  
A motor might accept only $[-12, 12]$ volts.  
But the controller may compute $u=20$V because of the accumulated integral term.

What happens?
\begin{itemize}
    \item Actuator outputs only $12$V.
    \item Integrator keeps increasing (windup).
    \item Controller thinks it is applying $20$V (internal model), but the plant sees only $12$V.
    \item As soon as the error becomes small, the huge integrator causes overshoot.
\end{itemize}

This leads to oscillations and long settling time —  
not due to bad $K$ or bad model, but due to ignoring actuator limits.


\subsection{Antiwindup correction}

The Fig.~\ref{fig:lqri_aw_block_diagram} shows full LQR block diagram, with both, integral action and antiwindup.
Antiwindup section is marked by orange color.
Complete algorithm is demonstrated in Fig.~\ref{fig:lqr_algo_integral_action_aw}.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.6]{../diagrams/lqr/lqr-lqri_aw_diagram.png}
    \caption{LQR with integral action and antiwindup block diagram}
    \label{fig:lqri_aw_block_diagram}
\end{figure}

First compute the tentative integral update:
\[
\hat{e}_{int}(n) = e_{int}(n-1) + K_i e(n)
\]

Compute the tentative (unsaturated) control:
\[
\hat{u}(n) = -Kx(n) + \hat{e}_{int}(n)
\]  

Apply actuator saturation:
\[
u(n) = \operatorname{clip}(\hat{u}(n), u_{min}, u_{max})
\]

Correct the integrator:
\[
e_{int}(n) = \hat{e}_{int}(n) - (\hat{u}(n) - u(n))
\]

The correction term zeroes out whenever the actuator is not saturated.

This works cleanly because the integral term lives in the \textbf{action space} ($K_i e$),
so saturation and projection are consistent and simple.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{../diagrams/lqr/lqr-lqri_aw_algorithm.png}
    \caption{LQR algorithm with integral action and antiwindup}
    \label{fig:lqr_algo_integral_action_aw}
\end{figure}


\section{Practical Implementation}

Below is a complete Python implementation of the LQR controller with integral action and antiwindup.

The constructor precomputes control matrices.  
The method \textbf{solve} augments matrices and solves DARE.  
The real-time loop calls \textbf{forward}, which requires:
\begin{itemize}
    \item desired state $x_r$,
    \item current state $x$,
    \item current integral state.
\end{itemize}

The output is control $u$ and updated integrator.



\clearpage
\begin{lstlisting}[style=python_style]
import numpy
import scipy

class LQRIDiscrete: 

    def __init__(self, a, b, q, r, antiwindup = 10**10):
        self.k, self.ki = self.solve(a, b, q, r)

        self.antiwindup = antiwindup

    def forward(self, xr, x, integral_action):
        # integral action
        error = xr - x

        integral_action_new = integral_action + self.ki@error

        #LQR controll law
        u_new = -self.k@x + integral_action
        
        #conditional antiwindup
        u = numpy.clip(u_new, -self.antiwindup, self.antiwindup)

        integral_action_new = integral_action_new - (u_new - u)

        return u, integral_action_new

    def solve(self, a, b, q, r):
        n = a.shape[0]  # system order
        m = b.shape[1]  # system inputs

        # augmented system
        a_aug = numpy.block([
            [a, numpy.zeros((n, n))],
            [numpy.eye(n), numpy.eye(n)]
        ])
        
        b_aug = numpy.vstack([b, numpy.zeros((n, m))])
        
        # augmented cost
        q_aug = numpy.block([
            [numpy.zeros((n, n)), numpy.zeros((n, n))],   
            [numpy.zeros((n, n)), q]
        ])

        p = scipy.linalg.solve_discrete_are(a_aug, b_aug, q_aug, r)
        k_aug = numpy.linalg.inv(r) @ (b_aug.T @ p)

        #truncated small elements
        k_aug[numpy.abs(k_aug) < 10**-10] = 0
        
        k  = k_aug[:, :n]
        ki = k_aug[:, n:]

        return k, ki
\end{lstlisting}



\clearpage
\section{Summary}

The Linear Quadratic Regulator is one of the most widely used controllers in modern control engineering.
Its strength comes from combining a mathematically optimal formulation with practical usability.
Once a state--space model is available, LQR provides a systematic way to compute all feedback gains at once,
even for large multivariable systems.

\subsection*{Applications}

LQR is a standard tool in fields where:
\begin{itemize}
    \item accurate state--space models exist,
    \item systems have multiple coupled states and inputs,
    \item fast, reliable, and stable control is required.
\end{itemize}

Typical applications include:
\begin{itemize}
    \item robotics and manipulators,
    \item quadrotors, drones, and aerospace vehicles,
    \item autonomous driving and vehicle dynamics control,
    \item inverted pendulums and balancing systems,
    \item power systems and energy control,
    \item any linearized model in engineering simulations.
\end{itemize}

\subsection*{Advantages}

LQR is popular because it offers:
\begin{itemize}
    \item \textbf{Uniqueness:} the solution to the Riccati equation is unique and globally optimal for the linear-quadratic problem.
    \item \textbf{Multivariable optimality:} all gains are solved simultaneously, avoiding the combinatorial tuning of PID.
    \item \textbf{Stability guarantees:} the resulting $K$ stabilizes the system whenever the standard LQR conditions are met.
    \item \textbf{Clear tuning philosophy:} $Q$ shapes state penalties, $R$ shapes effort penalties.
    \item \textbf{Computational efficiency:} Riccati equations are solved once offline; online computation is a single matrix multiplication.
    \item \textbf{Smooth control actions:} the quadratic penalty naturally avoids aggressive or discontinuous inputs.
\end{itemize}

\subsection*{Limitations}

Despite its strengths, LQR has several important limitations:
\begin{itemize}
    \item \textbf{Requires a linear state--space model.}  
          Nonlinear systems must be linearized or approximated.
    \item \textbf{Sensitive to modelling errors.}  
          Large unmodelled dynamics or unmodelled delays degrade performance.
    \item \textbf{No inherent integral action.}  
          Steady-state errors require explicit integrator augmentation.
    \item \textbf{No built-in actuator constraints.}  
          Saturation must be handled externally (antiwindup, MPC, etc.).
    \item \textbf{Not optimal for non-quadratic cost functions.}  
          The controller’s optimality holds only for quadratic penalties.
\end{itemize}

\subsection*{Final Remarks}

LQR remains a gold standard for linear control because it is:
\begin{itemize}
    \item theoretically elegant,
    \item computationally cheap,
    \item practically effective,
    \item easy to integrate with integral action and antiwindup,
    \item naturally suited to multivariable systems.
\end{itemize}

For systems with good linear models and strong coupling, LQR often provides a performance baseline that is
difficult to surpass using manual PID tuning.
